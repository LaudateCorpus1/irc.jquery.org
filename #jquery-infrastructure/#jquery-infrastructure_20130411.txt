[10:01:16] <ryanneufeld> clarkbox: you around?
[10:46:31] <clarkbox> hey ryanneufeld
[10:47:02] <ryanneufeld> oh, hey
[10:47:08] <ryanneufeld> so, this splunk storm thing
[10:47:11] <ryanneufeld> it's a thing eh
[10:47:15] <clarkbox> oh ya!
[10:47:19] <clarkbox> its good to get started
[10:47:23] <ryanneufeld> we got it
[10:47:34] <clarkbox> nice
[10:47:41] <clarkbox> its good for limited use cases
[10:47:43] <ryanneufeld> I'm just wondering, how hard is it to puppetize installing for this?
[10:47:59] <ryanneufeld> we have 11 servers I need to get it on
[10:48:10] <clarkbox> but you can't install apps, or customize things as much as the shrink-wrapped product
[10:48:20] <clarkbox> we have it pauperized 
[10:48:31] <clarkbox> though I'm not sure if storm uses the same forwarders?
[10:48:41] <ryanneufeld> seems to
[10:48:51] <ryanneufeld> wget -O splunkforwarder-5.0.2-149561-linux-2.6-x86_64.rpm 'http://www.splunk.com/page/download_track?file=5.0.2/universalforwarder/linux/splunkforwarder-5.0.2-149561-linux-2.6-x86_64.rpm&ac=&wget=true&name=wget&typed=releases'
[10:48:56] <ryanneufeld> universal forwarder?
[10:48:59] <clarkbox> ya thats it
[10:49:10] <clarkbox> so you'll just need to puppet that, and some config files
[10:49:38] <clarkbox> or you could puppet CLI commands
[10:50:30] <ryanneufeld> I suppose I could look at how you did it for infra
[10:50:47] <ryanneufeld> wasn't sure if you had a special build, or what the process was
[10:51:24] <clarkbox> nah its regular build
[10:51:33] <clarkbox> nothing has changed since that version
[10:52:39] <clarkbox> I'm sure you have seen thisâ€¦ http://docs.splunk.com/Documentation/Storm/latest/User/Setupauniversalforwarderonnix
[10:52:53] <ryanneufeld> yeah
[10:53:14] <clarkbox> how much data/day do you expect to index?
[11:00:04] <ryanneufeld> dunno
[11:00:15] <ryanneufeld> I'm hoping we can get our code down to only being 404s and errors being logged
[11:00:42] <ryanneufeld> but right now our Tech debt means our codebase is a bit of a shit show (still a bajillion times better than ever)
[11:10:28] <clarkbox> cool. let me know how it goes, or if you have any questions.
[11:15:28] <ryanneufeld> thanks clarkbox
[11:15:29] <ryanneufeld> I will
[11:41:23] <clarkbox> oh ryanneufeld heads upâ€¦ on jquery we use the "deployment server" feature for managing forwarders. you will not want to use that. instead, follow that docs guide. you'll figure it out if not already.
[11:44:24] <ryanneufeld> Thanks clarkbox
[12:19:40] <gnarf> clarkbox: what was that port i needed to open up on the helix again?
[12:19:48] <clarkbox> 8089
[13:43:27] <jqcommit> 01[13infrastructure01] 15gnarf37 pushed 1 new commit to 06puppet-stage: 02https://github.com/jquery/infrastructure/commit/1a83bea50fd73b102ef874a3b9325d1a767c2f7c
[13:43:27] <jqcommit> 13infrastructure/06puppet-stage 141a83bea 15Corey Frang: meetings.jquery.org stubs in puppet
[13:43:40] <gnarf> rworth: around here too?
[13:44:01] <gnarf> rworth: we already have jquery.org base cert, just fyi - but we might wanna upgrade to the green bar (EV) cert anyway
[13:44:07] <gnarf> cc danheberden
[13:44:51] <gnarf> rworth: http://stage.meetings.jquery.org/
[13:45:22] <gnarf> rworth: also, one other step i forgot when we were just talking --- webhooks in github admin
[14:12:32] <jqcommit> 01[13infrastructure01] 15gnarf37 pushed 1 new commit to 06puppet-master: 02https://github.com/jquery/infrastructure/commit/2664b269bf36a953857b83d6dac1540591ad193c
[14:12:32] <jqcommit> 13infrastructure/06puppet-master 142664b26 15Corey Frang: Adding postfix to wordpress server so it can email
[14:12:42] <jqcommit> 01[13infrastructure01] 15gnarf37 pushed 1 new commit to 06puppet-stage: 02https://github.com/jquery/infrastructure/commit/59bec703e2d52e82baa8d99b92cab62190d4e463
[14:12:42] <jqcommit> 13infrastructure/06puppet-stage 1459bec70 15Corey Frang: Adding postfix to wordpress server so it can email
[14:22:59] <rworth> gnarf yeah, I wanna upgrade to the green bar (EV) for .org
[14:23:19] <rworth> we talked about that when we bought the current cert for the short-term caus it was quickest to get
[14:24:02] <gnarf> also rworth, last month from all of our CDN's we delivered 370terabytes
[14:24:14] <gnarf> sadly, its really hard to get individual reports
[14:24:19] <gnarf> but i can try to watch that #
[14:25:03] <rworth> I wanna say that seems a very different number than what we gave NetDNA a few months back for them to do a quote
[14:25:12] <rworth> was that you or danheberden that gave me that info?
[14:25:22] <gnarf> not sure
[14:25:25] <rworth> hmm
[14:25:38] <rworth> maybe it was a graph from splunk?
[14:26:04] <gnarf> clarkbox: does someone have an edgecast plugin for splunk?
[14:26:13] <gnarf> i can get you api access
[14:26:19] <gnarf> or whatever
[14:26:24] <gnarf> might be nice to get this into splunk
[14:29:51] <gnarf> ryanneufeld danheberden PS - running apt-get upgrade system wide
[14:30:00] <ryanneufeld> gnarf: (y)
[14:30:38] <gnarf> should kill those zabbix notices for like a few days at least
[14:30:52] <gnarf> lets try to stay on top of em a bit more as a team?
[14:45:27] <danheberden> rworth: different in a good way?
[14:45:28] <danheberden> or a bad way?
[14:45:52] <rworth> danheberden I think I would've remembered such a big number
[14:46:09] <rworth> so that's not great for wanting to have gotten an accurate quote from them, but I could be remembering wrong
[14:46:23] <rworth> let me dig up what I sent them
[14:46:46] <danheberden> rworth: oh, not the $ for certs
[14:46:47] <danheberden> hah
[14:46:54] <rworth> oh, that was the numbers we sent them
[14:46:55] <danheberden> rworth: yeah, didn't we get like 29T from (mt)
[14:46:57] <rworth> no worries
[14:47:02] <danheberden> oh cool
[14:47:26] <danheberden> gnarf: so headers didn't have an improvement?
[14:48:51] <rworth> these were post headers numbers, no?
[14:48:58] <gnarf> yeah
[14:49:03] <gnarf> i can dig up pre-cache headers
[14:49:06] <rworth> what was it before?
[14:49:09] <gnarf> like same month last year?
[14:49:10] <rworth> gnarf that would be great
[14:49:12] <rworth> yeah
[14:49:15] <rworth> or like July
[14:49:25] <rworth> caus that change was circa Aug-Oct
[14:50:41] <gnarf> hrm
[14:50:45] <gnarf> this query is taking a long time
[14:50:47] <gnarf> brb
[14:50:55] <gnarf> perhaps its not cached ;)
[14:52:53] <gnarf> all of the months before july i checked are ~215 TB of data
[14:53:03] <gnarf> so seems to have had zero effect
[14:53:46] <gnarf> this could just be our rising popularity tho
[15:03:01] <rworth> hmm
[15:03:09] <danheberden> i knew i shouldn't have put those pictures of me on the cdn
[15:03:14] <danheberden> sorry y'all
[15:03:15] <rworth> that's it!
[15:03:30] <danheberden> so we're using less data via script/image resources
[15:03:36] <danheberden> and sending proper headers
[15:03:40] <danheberden> yet we're sending more data
[15:34:44] <gnarf> yeah
[15:34:50] <gnarf> doesn't make much sense does it
