[00:38:45] <Cork> jquery.com seams to be down
[00:39:14] <Cork> <b-ot> Cork: Â¯\_(ãƒ„)_/Â¯ It's not just you! http://jquery.com is down! (ETIMEDOUT)
[03:33:19] <arschmitz> Cork: i just texted our infra guys
[03:33:44] <Cork> thx
[04:32:20] <scott_gonzalez> gnarf: ^
[04:36:05] <scott_gonzalez> ryanneufeld: ^
[04:36:22] <scott_gonzalez> danheberden: ^
[05:37:39] <kborchers> scott_gonzalez: Cork arschmitz i am working on this with s5fs now
[05:52:17] <s5fs> scott_gonzalez: call directly next time please :D
[05:52:29] <s5fs> apologize if you did call and i'm just a deep sleeper haha
[06:10:22] <kborchers> scott_gonzalez: Cork arschmitz sites are back up thanks to s5fs 
[06:10:55] <arschmitz> s5fs: Thanks you!
[06:13:01] <s5fs> arschmitz: Can you please test the sites/services you care about?
[06:14:21] <arschmitz> s5fs: everything i see is up
[06:14:32] <s5fs> arschmitz: Thanks!
[06:37:27] <s5fs> looks like jq01 went down just before midnight, making this a six hour outage
[06:37:31] <s5fs> grr
[06:38:26] <scott_gonzalez> s5fs: Yup, I called :-)
[06:39:11] <s5fs> scott_gonzalez: haha crap
[06:39:22] <s5fs> i guess i like the ring of Kris' phone better, sorry about that
[06:39:40] <scott_gonzalez> heh, no problem
[06:39:46] <s5fs> anyways, once i get root cause i'll update the channel
[06:39:52] <s5fs> back to bed for now, later skaters
[08:29:38] <gnarf> we really don't know WHY yet
[08:29:57] <gnarf> but the working theory is that we are going way over a lower network bandwidth limit on this new machine
[08:30:16] <gnarf> and they shut it out
[08:30:29] <gnarf> we aren't sure if this is true or not
[08:30:34] <gnarf> gonna let s5fs dig yet
[09:19:54] <jqcommit> [13infrastructure] 15scottgonzalez opened issue #283: Bandwidth Breakdown 02https://github.com/jquery/infrastructure/issues/283
[09:23:32] <jqcommit> [13infrastructure] 15ryanneufeld closed issue #283: Bandwidth Breakdown 02https://github.com/jquery/infrastructure/issues/283
[09:24:57] <jqcommit> [13infrastructure] 15ryanneufeld reopened issue #283: Bandwidth Breakdown 02https://github.com/jquery/infrastructure/issues/283
[09:25:42] <jqcommit> [13infrastructure] 15ryanneufeld closed issue #283: Bandwidth Breakdown 02https://github.com/jquery/infrastructure/issues/283
[09:26:22] <jqcommit> [13infrastructure] 15scottgonzalez reopened issue #283: Bandwidth Breakdown 02https://github.com/jquery/infrastructure/issues/283
[09:29:28] <ryanneufeld> scott_gonzalez: what exactly is it you're looking for? the CDN has a fair bit of data.
[09:29:30] <ryanneufeld> https://cp.maxcdn.com/reporting/zones
[09:29:40] <scott_gonzalez> I want (mt) bandwidth stats.
[09:29:48] <scott_gonzalez> Not MaxCDN.
[09:29:55] <ryanneufeld> ah
[09:30:02] <ryanneufeld> you didn't say that in your issue
[09:30:09] <ryanneufeld> why is there even an issue for this?
[09:30:37] <scott_gonzalez> That's why I said sites, not CDN ;-)
[09:30:46] <ryanneufeld> "sites" is vague
[09:30:49] <scott_gonzalez> Because it's supposedly what caused the outage.
[09:30:59] <scott_gonzalez> And 1TB for sites sounds high.
[09:31:21] <scott_gonzalez> We should be able to reduce that, but without knowing where the bandwidth is being used, that's hard to do.
[09:31:40] <ryanneufeld> splunk used to tell us that
[09:31:43] <ryanneufeld> but now I can't seem to find it
[09:32:15] <ryanneufeld> and i don't have full access to the MT stuff any way
[09:33:18] <ryanneufeld> scott_gonzalez: who told you that was the issue?
[09:33:55] <scott_gonzalez> Kris said Adam said (mt) said so :-P
[09:34:04] <ryanneufeld> ah
[09:34:42] <scott_gonzalez> Also see gnarf's comment above, which indicates that was the problem, though he wasn't 100% sure at the time.
[09:34:54] <ryanneufeld> I don't have that comment in my history
[09:35:08] <scott_gonzalez> ah, one sec
[09:35:17] <ryanneufeld> totally flying blind man
[09:35:23] <scott_gonzalez> http://irc.jquery.org/%23jquery-infrastructure/default_%23jquery-infrastructure_20140819.log.html#t08:29:57
[09:36:23] <ryanneufeld> ah
[09:36:50] <ryanneufeld> the last thing i saw was s5fs saying he'd update the channel
[09:37:05] <ryanneufeld> must have been switching compys when that happened
[09:37:19] <scott_gonzalez> Yeah, he never came back with an update, but kborchers said he talked to him.
[09:37:38] <ryanneufeld> I'm honestly getting sick of the MT bullshit
[09:38:00] <ryanneufeld> seems every time we have an issue, it's with MT
[09:38:25] <ryanneufeld> I understand we're not paying for it. But man, I'd be even more upset if we were
[09:40:11] <ryanneufeld> gnarf: shouldn't we be getting a warning when we're close to a bandwidth limit any way?
[09:41:54] <ryanneufeld> scott_gonzalez: looking at maxcdn's graphs (as it's all I have) I'm going to assume it's code.jquery.com
[09:42:08] <ryanneufeld> 977.14 TB of outbound bandwidth
[09:42:21] <ryanneufeld> nothing else even comes close
[09:42:24] <scott_gonzalez> That barely touches (mt) though.
[09:42:43] <scott_gonzalez> Only on a CDN cache miss.
[09:42:56] <scott_gonzalez> Which would go to codeorigin.jquery.com
[09:43:18] <ryanneufeld> .2% of hits are misses
[09:43:43] <ryanneufeld> 1.95428 TB
[09:44:05] <ryanneufeld> of cache misses
[09:44:34] <ryanneufeld> so, yeah, 'barely' touches it, but when you're talking nearly a PB of data
[09:44:40] <ryanneufeld> 'barely' is relative
[09:44:59] <ryanneufeld> of course my assertion is based on the assumption that my math is correct
[09:45:26] <scott_gonzalez> Wow, I would not have expected .2% misses
[09:45:52] <scott_gonzalez> I'm going to guess that a large portion of those misses are 404
[09:46:16] <ryanneufeld> I dunno
[09:46:25] <ryanneufeld> I don't know where splunk landed
[09:49:10] <ryanneufeld> mrsparkle is running it looks like... 
[09:49:11] <ryanneufeld> ython -O /opt/splunk/lib/python2.7/site-packages/splunk/appserver/mrsparkle/root.py restart
[10:03:59] <gnarf> scott_gonzalez: i think you want something like https://splunk.jquery.com:8000/en-US/app/search/flashtimeline?q=search%20sourcetype%3D%22access_combined%22%20host!%3D%22codeorigin.jquery.com%22%20%7C%20timechart%20sum(bytes)%20by%20host&earliest=-24h%40h&latest=now&c.chart=line&c.title=&c.stack=none&c.split=false&c.nulls=gaps&c.legend=top&c.y.min=&c.y.max=&c.y.scale= this?
[10:04:35] <gnarf> you can change the host != to a specific host and timechart by uri instead to see the heaviest pages/assests loading
[10:04:54] <scott_gonzalez> gnarf: Thanks.
[10:05:39] <jqcommit> [13infrastructure] 15scottgonzalez closed issue #283: Bandwidth Breakdown From MediaTemple 02https://github.com/jquery/infrastructure/issues/283
[10:05:46] <scott_gonzalez> Heh, well you can see where the server died on that graph :-P
[10:06:32] <ryanneufeld> gnarf: did MT give us a warning about this?
[10:06:40] <gnarf> demos.jquerymobile.com is not on jq01 either
[10:07:19] <scott_gonzalez> gnarf: Yeah, I figured based on that data ;-)
[10:07:52] <gnarf> http://cl.ly/image/3g3k470t092c
[10:07:55] <scott_gonzalez> There was a spike of "OTHER" data right before the crash.
[10:08:32] <scott_gonzalez> So the bandwidth is more like 2TB per month, right?
[10:08:40] <ryanneufeld> which server does codeorigin live on? jq01?
[10:08:49] <scott_gonzalez> That would make a lot more sense than 1TB per day for non-CDN.
[10:09:21] <ryanneufeld> scott_gonzalez: yeah, it looks like it's a monthly
[10:09:28] <ryanneufeld> but if you look at the math I did earlier...
[10:09:46] <gnarf> scott_gonzalez: add limit=50 if you want to get more granyular than "other"
[10:09:59] <gnarf> to the end of the timechart
[10:17:01] <gnarf> it actually looks like there was a big traffic spike like an attack here: http://citadel.jquery.com/zabbix/charts.phphttp://citadel.jquery.com/zabbix/chart2.php?graphid=406&period=3600&stime=20140818225347&updateProfile=1&profileIdx=web.screens&profileIdx2=406&sid=04b2d968070894fb&width=1536
[10:17:05] <gnarf> http://citadel.jquery.com/zabbix/chart2.php?graphid=406&period=3600&stime=20140818225347&updateProfile=1&profileIdx=web.screens&profileIdx2=406&sid=04b2d968070894fb&width=1536
[10:24:33] <scott_gonzalez> gnarf: The big spike before the server died was for content.origin.jquery.com
[10:24:58] <scott_gonzalez> Maybe look at the requests for that site between midnight and 4:00a.
[10:25:19] <scott_gonzalez> Looks like the requests were from 1:00a - 2:00a, but kinda hard to be accurate with that graph.
[10:25:25] <gnarf> scott_gonzalez: also not on jq01 afaik
[10:25:34] <scott_gonzalez> hmm...
[10:25:40] <gnarf> content.origin.jquery.com. 3600	IN	A	64.13.224.24
[10:25:44] <scott_gonzalez> I wonder why there was such a big spike though.
[10:25:49] <gnarf> jq01.jquery.com.	300	IN	A	205.186.138.13
[10:26:08] <gnarf> scott_gonzalez: you can switch it to host="content.origin.jquery.com" | timechart sum(bytes) by uri
[10:26:38] <gnarf> sourcetype="access_combined" host="content.origin.jquery.com" | timechart sum(bytes) by uri limit=500
[10:27:02] <gnarf> or maybe thats not what i was looking for
[10:27:03] <gnarf> hrm
[10:28:33] <scott_gonzalez> So we have a half gig video on http://events.jquery.org/2010/boston/todd-parker/todd-parker-desktop.m4v
[10:28:58] <scott_gonzalez> Sorry, content.origin.jquery.com/2010/boston/todd-parker/todd-parker-desktop.m4v
[10:29:12] <scott_gonzalez> That was the big spike.
[10:29:15] <gnarf> scott_gonzalez yeah
[10:29:42] <gnarf> not much of a concern, that should only really be hit by the cdn
[10:29:55] <gnarf> its not being done very often
[12:46:54] <gnarf> s5fs you around now?
[12:47:03] <gnarf> kborchers: or you?
[12:47:55] <kborchers> i am but about to run an errand
[12:47:57] <kborchers> what's up?
[12:48:29] <gnarf> kborchers: just wondering if you know if we've tried to call/talk to alex yet?
[12:48:37] <gnarf> or if you think we should
[12:48:55] <kborchers> gnarf: i am waiting on details from s5fs on exactly what happened this morning and was planning to talk to him
[13:21:40] <s5fs> I've been in contact with MT, the ball is in their court.
